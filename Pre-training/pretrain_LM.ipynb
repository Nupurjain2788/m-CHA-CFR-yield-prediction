{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Disable RDKit warning messages\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Add custom path for imports\n",
    "sys.path.append('./fastai1/')\n",
    "from utils import *\n",
    "\n",
    "# Print the current working directory\n",
    "current_path = os.getcwd()\n",
    "print(f\"Current working directory: {current_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a custom tokenizer\n",
    "\n",
    "# Don't include the defalut specific token of fastai, only keep the padding token\n",
    "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
    "TK_MAJ,TK_UP,TK_REP,TK_WREP = 'xxmaj','xxup','xxrep','xxwrep'\n",
    "defaults.text_spec_tok = [PAD]\n",
    "\n",
    "\n",
    "\n",
    "special_tokens = ['[BOS]', '[C@H]', '[C@@H]','[C@]', '[C@@]','[C-]','[C+]', '[c-]', '[c+]','[cH-]',\n",
    "                   '[nH]', '[N+]', '[N-]', '[n+]', '[n-]' '[NH+]', '[NH2+]', '[O-]', '[S+]', '[s+]',\n",
    "                   '[S-]', '[O+]', '[SH]', '[B-]','[BH2-]', '[BH3-]','[b-]','[PH]','[P+]', '[I+]', \n",
    "                   '[Si]','[SiH2]', '[Se]','[SeH]', '[se]', '[Se+]', '[se+]','[te]','[te+]', '[Te]',\n",
    "                   '[Pd]' , '[Ag]','[Cs]','[Li]','[K]','[Na]', '[N@]', '[N@@]', '[S@+]''[K+]', '[Ni+2]',\n",
    "                   '[Mg]','[Li+]', '[Cl-]', '[Ni]','[Cs+]', '[Cu+2]', '[Zn+2]', '[Al]', '[Cu]']\n",
    "\n",
    "\n",
    "\n",
    "class MolTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang = 'en', special_tokens = special_tokens):\n",
    "        self.lang = lang\n",
    "        self.special_tokens = special_tokens\n",
    "        \n",
    "    def tokenizer(self, smiles):\n",
    "        # add specific token '[BOS]' to represetences the start of SMILES\n",
    "        smiles = '[BOS]' + smiles\n",
    "        regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
    "        char_list = re.split(regex, smiles)\n",
    "        tokens = []\n",
    "        \n",
    "        if self.special_tokens:\n",
    "            for char in char_list:\n",
    "                if char.startswith('['):\n",
    "                    if char in special_tokens:\n",
    "                        tokens.append(str(char))\n",
    "                    else:\n",
    "                        tokens.append('[UNK]')\n",
    "                else:\n",
    "                    chars = [unit for unit in char]\n",
    "                    [tokens.append(i) for i in chars]                    \n",
    "        \n",
    "        if not self.special_tokens:\n",
    "            for char in char_list:\n",
    "                if char.startswith('['):\n",
    "                    tokens.append(str(char))\n",
    "                else:\n",
    "                    chars = [unit for unit in char]\n",
    "                    [tokens.append(i) for i in chars]\n",
    "                \n",
    "        #fix the 'Br' be splited into 'B' and 'r'\n",
    "        if 'B' in tokens:\n",
    "            for index, tok in enumerate(tokens):\n",
    "                if tok == 'B':\n",
    "                    if index < len(tokens)-1: # make sure 'B' is not the last character\n",
    "                        if tokens[index+1] == 'r':\n",
    "                            tokens[index: index+2] = [reduce(lambda i, j: i + j, tokens[index : index+2])]\n",
    "        \n",
    "        #fix the 'Cl' be splited into 'C' and 'l'\n",
    "        if 'l' in tokens:\n",
    "            for index, tok in enumerate(tokens):\n",
    "                if tok == 'l':\n",
    "                    if tokens[index-1] == 'C':\n",
    "                            tokens[index-1: index+1] = [reduce(lambda i, j: i + j, tokens[index-1 : index+1])]\n",
    "        return tokens    \n",
    "    \n",
    "    def add_special_cases(self, toks):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc957b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# Set CUDA environment variable directly for device 4\n",
    "%env CUDA_VISIBLE_DEVICES=4\n",
    "\n",
    "# Define the device (CUDA if available, otherwise CPU)\n",
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define result paths\n",
    "result_path = Path('./results')\n",
    "name = 'C-H-activation'\n",
    "path = result_path / name\n",
    "path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Create directory for models\n",
    "mdl_path = path / 'models'\n",
    "mdl_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "csv_file = 'pretrain-4a-merged.csv'\n",
    "if Path(csv_file).exists():\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(\"Dataset loaded successfully:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"Error: File '{csv_file}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train = df.iloc[:-23653]\n",
    "valid = df.iloc[-23653:]\n",
    "print(f\"Train shape: {train.shape}, Validation shape: {valid.shape}\")\n",
    "\n",
    "# Function to randomize SMILES strings\n",
    "def randomize_smiles(smiles):\n",
    "    \"\"\"Randomizes the atom order in a SMILES string.\"\"\"\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    ans = list(range(m.GetNumAtoms()))\n",
    "    np.random.shuffle(ans)\n",
    "    nm = Chem.RenumberAtoms(m,ans)\n",
    "    return Chem.MolToSmiles(nm, canonical=False, isomericSmiles=True, kekuleSmiles=False)\n",
    "\n",
    "\n",
    "# Function to randomize reaction SMILES\n",
    "def randomize_rxn(rxn):\n",
    "    \"\"\"Randomizes the atom order in a reaction SMILES string.\"\"\"\n",
    "    precursors, product = rxn.split('>>')\n",
    "    precursors_list = precursors.split('.')\n",
    "    \n",
    "    randomized_precursors = [randomize_smiles(precursor) for precursor in precursors_list]\n",
    "    randomized_product = randomize_smiles(product)\n",
    "    \n",
    "    return f\"{'.'.join(randomized_precursors)}>>{randomized_product}\"\n",
    "\n",
    "# Function to augment SMILES data\n",
    "def smiles_augmentation(df, N_rounds, smiles_col=\"smiles\"):\n",
    "    dist_aug = {col_name: [] for col_name in df}\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        for j in range(N_rounds):\n",
    "            #dist_aug[smiles_col].append(randomize_smiles(df.iloc[i][smiles_col]))\n",
    "            dist_aug[smiles_col].append(randomize_smiles(df.iloc[i][smiles_col]))\n",
    "            \n",
    "            #dist_aug['canonical'].append('no')\n",
    "\n",
    "    df_aug = pd.DataFrame.from_dict(dist_aug)\n",
    "    \n",
    "    #merge with original df\n",
    "    df = pd.concat([df, df_aug], sort=False).reset_index(drop=True)\n",
    "    #shuffle the data\n",
    "    df = df.reindex(np.random.permutation(df.index))\n",
    "    return pd.DataFrame.from_dict(df).drop_duplicates(smiles_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df950b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = smiles_augmentation(train, 4)\n",
    "valid_aug = smiles_augmentation(valid, 4)\n",
    "\n",
    "# Display the shapes of the augmented datasets\n",
    "print(f\"Augmented Train Shape: {train_aug.shape}\")\n",
    "print(f\"Augmented Validation Shape: {valid_aug.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from functools import partial\n",
    "\n",
    "# Initialize the Tokenizer with a custom molecular tokenizer and special tokens\n",
    "tok = Tokenizer(partial(MolTokenizer, special_tokens=special_tokens), n_cpus=6, pre_rules=[], post_rules=[])\n",
    "\n",
    "# Set batch size\n",
    "bs = 128\n",
    "\n",
    "# Prepare the data for training and validation using the augmented datasets\n",
    "data = TextLMDataBunch.from_df(\n",
    "    path, train_aug, valid_aug, bs=bs, tokenizer=tok, \n",
    "    chunksize=50000, text_cols=0, max_vocab=60000, include_bos=False\n",
    ")\n",
    "\n",
    "# Show a batch of data for inspection\n",
    "data.show_batch()\n",
    "\n",
    "# Save the DataBunch to disk\n",
    "data.save(f'{name}_databunch')\n",
    "\n",
    "# Output the size of vocabulary and number of training examples\n",
    "print(f\"Vocabulary size: {len(data.vocab.itos)}\")\n",
    "print(f\"Training dataset size: {len(data.train_ds)}\")\n",
    "\n",
    "# Load the saved DataBunch for further processing\n",
    "data_lm = load_data(path, f'{name}_databunch', bs=bs)\n",
    "\n",
    "# Create a language model learner with the AWD_LSTM architecture\n",
    "learner = language_model_learner(data_lm, AWD_LSTM, drop_mult=1, pretrained=False)\n",
    "\n",
    "# Display model architecture\n",
    "print(learner.model)\n",
    "\n",
    "# Set learning rate and scale it based on batch size\n",
    "lr = 3e-3\n",
    "lr *= bs / 48  # Scale learning rate by batch size\n",
    "\n",
    "# Unfreeze the model and train it using one cycle learning rate policy\n",
    "learner.unfreeze()\n",
    "learner.fit_one_cycle(100, lr, moms=(0.8, 0.7))\n",
    "\n",
    "# Define filenames for saving the model and vocabulary\n",
    "lm_fns = [f'{name}_100_wt', f'{name}_100_vocab']\n",
    "\n",
    "# Save the model weights (excluding optimizer) and vocabulary\n",
    "learner.save(lm_fns[0], with_opt=False)\n",
    "learner.data.vocab.save(mdl_path / (lm_fns[1] + '.pkl'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
